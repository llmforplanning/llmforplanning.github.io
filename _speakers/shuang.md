---
# Name of the speaker
name: Shuang Li

# Link to the speaker's webpage
webpage: https://shuangli59.github.io/

# Primary affiliation of the speaker
affil: Stanford
# Link to the speaker's primary affiliation
affil_link: https://shuangli59.github.io/

# An image of the speaker (square aspect ratio works the best) (place in the `assets/img/speakers` directory)
img: shuang.jpg

---

<!-- Whatever you write below will show up as the speaker's bio -->

<!-- Shuang earned her Ph.D. degree from MIT in Aug 2023. She will start as an Assistant Professor at the University of Toronto and Vector Institute in Fall 2024. Shuang is interested in developing AI systems that generalize to a wide range of novel tasks and continually learn from the environment. Her research explores methods to incorporate compositionality into deep learning models, giving rise to stronger generalization abilities for solving more challenging novel tasks. Her research involves Generative Modeling, Embodied AI, and Vision-Language Understanding. Shuang is a recipient of the Meta Research Fellowship, Adobe Research Fellowship, MIT Seneff-Zue CS Fellowship, EECS Rising Star, ICML Outstanding Reviewer, and best and outstanding paper awards at NeurIPS workshops. -->

**Bio**: 
Shuang Li is a Postdoctoral Researcher at Stanford and earned her Ph.D. from MIT. Her research focuses on generative modeling and robot learning. She is interested in developing AI systems that generalize to a wide range of novel tasks and continually learn from the environment. In particular, she investigates methods to enhance the generalization capabilities of deep learning models, enabling them to tackle more challenging and novel tasks. Shuang is a recipient of CIFAR AI Chair, Meta Research Fellowship, Adobe Research Fellowship, MIT Seneff-Zue CS Fellowship, EECS Rising Star, ICML Outstanding Reviewer, and best and outstanding paper awards at NeurIPS workshops.

**Abstract**:
Recent advances in vision and language models are transforming decision-making processes in robotics and automation. In this talk, we present several innovative frameworks that leverage language models as high-level planners to break down complex tasks into manageable subtasks, while employing video generation models as low-level controllers to execute actions in dynamic environments. We also introduce a novel unified video-and-action method that overcomes the limitations of directly applying video generation techniques to robotics and enables rapid policy inference. This integrated approach significantly enhances both the interpretability and efficiency of robotic systems.
